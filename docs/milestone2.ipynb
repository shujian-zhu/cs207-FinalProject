{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "Evaluation of derivatives is integral to many machine learning methods. For this purpose, two main methods could be used: symbolical and numerical differentiation. Symbolical differentiation, though straightforward, its implementation requires complex expression manipulation in computer algebra systems, making it very costly to evaluate; the method is also limited to closed-form input expressions. On the other hand, numerical differentiation computes the function derivative by approximating it using small values of step size h; though numerical simpler and faster than symbolic diffferentiation, it suffers from stability issues and round-off or truncation errors.\n",
    "\n",
    "To address the weaknesses of both these methods, automatic differentiation (AD) was introduced. Since, it has been applied in different areas, such as  engineering design optimization, structural mechanics, and atmospheric sciences; its application to machine learning methods popularised AD. Therefore, due to the important role AD plays in many scientific fields, we introduce a python package that provides user-friendly methods for performing forward-mode AD. Our package supports the evaluation of first derivatives of functions defined by user at given input value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Background\n",
    "All numerical computation can be seen as a combination of elementary operations for which the derivatives are known. The derivatives of the overall composition can be found by combining the derivatives of elementary operations through the chain rule. Such elementary functions include arithmetic operations (addition, subtraction, multiplication, division), sign switch, and functions such as exponential, logarithm, and the trigonometric (e.g. sin(x), cos(x)). Traces of these elementary operations can be represented by a trace table or a computational graph. Trace table are originally used to trace the value of variables as each line of code is executed. As an example of this flow, Table 1 shows the evaluation trace of elementary operations of the computation f(x<sub>1</sub>) = ln(x<sub>1</sub>) + 3 * x<sub>1</sub> and Figure 1 gives an example of a graphic representation of function f(x<sub>1</sub>) by its elementary operations. \n",
    "\n",
    "| Trace       | Elementary function | Current Function Value | Function Derivative |\n",
    "| ------------- |:-------------:|:-------------:|:-------------:|\n",
    "| X1      | X1            |  c             | 1  |\n",
    "| X2      | ln(X1)            | ln(c)      | 1/c |\n",
    "| X3      | 3 * X1            |  3c        | 3   |\n",
    "| X4      | X2 + X3             | ln(c) + 3c | 1/c + 3 |\n",
    "\n",
    "![alt text](sample_trace_graph.png \"Figure 1\")\n",
    "\n",
    "The forward mode of AD starts from the input value and compute the derivative of intermediate variables with respect to the input value. Applying the chain rule to each elementary operation in the forward primal trace, we generate the corresponding derivative trace, which gives us the derivative in the final variable. Forward  mode AD can also be viewed as evaluating a function using dual numbers, which are defined as a+b*&epsilon;, where a, b &#1013; R and &epsilon; is a nilpotent number such that &epsilon;^2 = 0 and &epsilon; &ne; 0. It can be shown that the coefficient of &epsilon; after evaluating a function is exactly the derivative of that function, which also works for chain rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Installation\n",
    "Th package uses a setup.py file for a simple installation, you need to to run the following command in the package main folder:\n",
    "\n",
    "```\n",
    "pip install .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. How to Use AutoDiff \n",
    "In this section, we demo the the AutoDiff package and its capabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import AutoDiff as ad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AutoDiff package can be used in two different ways:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Pass the function to AD()\n",
    "You can pass disrectly a \\[function, a variable label, and variable value\\] to AD as follows:\n",
    "\n",
    "```\n",
    "AD(function: str, variable_label:str, variable_value: float): -> (value, derivative)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val:  11 \n",
      "der:  -1\n",
      "AD Object: Value = 11.000, Derivative =-1.000\n"
     ]
    }
   ],
   "source": [
    "## y = f(x)\n",
    "y = ad.AD('-x**3 + 2*x**2 + 3*x + 5', 'x', 2)\n",
    "print(\"val: \", y.val,\"\\nder: \", y.der)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val:  11 \n",
      "der:  -1\n",
      "AD Object: Value = 11.000, Derivative =-1.000\n"
     ]
    }
   ],
   "source": [
    "## z = f(w)\n",
    "z = ad.AD('-w**3 + 2*w**2 + 3*w + 5', 'w', 2)\n",
    "print(\"val: \", z.val,\"\\nder: \", z.der)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AD Object: Value = 1.000, Derivative =-0.000\n"
     ]
    }
   ],
   "source": [
    "## Elementary function example  \n",
    "y = ad.AD('cos(x)', 'x', 0)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Create an AD_Object()\n",
    "Alternatively, you can delcare an AD_Object(value) and use it as a variable; the AD object will store the value and derivative with each subsequent elementary operation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val:  11 \n",
      "der:  -1\n",
      "AD Object: Value = 11.000, Derivative =-1.000\n"
     ]
    }
   ],
   "source": [
    "x = ad.AD_Object(2)\n",
    "y = -x**3 + 2*x**2 + 3*x + 5\n",
    "print(\"val: \", y.val,\"\\nder: \", y.der)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val:  11 \n",
      "der:  -1\n",
      "AD Object: Value = 11.000, Derivative =-1.000\n"
     ]
    }
   ],
   "source": [
    "w = ad.AD_Object(2)\n",
    "z = -w**3 + 2*w**2 + 3*w + 5\n",
    "print(\"val: \", z.val,\"\\nder: \", z.der)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AD Object: Value = 1.000, Derivative =-0.000\n"
     ]
    }
   ],
   "source": [
    "## Elementary function example  \n",
    "x = ad.AD_Object(0)\n",
    "y = ad.cos(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Supported Elementary Functions\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following elementary functions are currently supported by the AutoDiff package:\n",
    "- addition(+), subtraction(-), multiplication(*) and division(/)\n",
    "- power (can be called by pow() or using **)\n",
    "- sine (sin), cosine (cos), tangent (tan)\n",
    "- natural log (ln), exponential (**note: exp should be called by e() and not exp()**)\n",
    "\n",
    "We will add more functions as the project proceeds; we envision having most functions implemented in the math module of the standard python library.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Software Organization\n",
    "The package has the following directory structure:\n",
    "- README.md\n",
    "- LICENSE.md\n",
    "- setup.py\n",
    "- requirements.txt\n",
    "- test/ (using CodeCov and TravisCI)\n",
    "    - \\__init\\__.py\n",
    "    - test_AutoDiff.py\n",
    "- docs\n",
    "    - milestone1.md\n",
    "    - milestone2.ipynb\n",
    "    - sample_trace_graph.png\n",
    "- AutoDiff/\n",
    "    - \\__init\\__.py\n",
    "    - AutoDiff_Class.py\n",
    "\n",
    "The package uses standard .py standalone files and setup.py for package installation; it will be also distributed through\n",
    "conda. Finallt, we implemented 31 tests in the test directory; the pytest framework is used, so the tests can be simply run by invoking pytest in that folder. TravisCI and CodeCov are used for integeration and coverage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Implementation\n",
    "Under the **AutoDiff** package, we implemented an Automatic Differentiation Class (*AD*) that stores and tracks the value of the given function and its gradient. This class calls another class, *AD_Object()*, which intitializes the given variable of the function as an automatic differentiation object.  \n",
    "\n",
    "## 6.1. Current implementation\n",
    "### 6.1.1. AD_Object class\n",
    "The *AD_Object* class takes a numeric value as an input and intitializes an *AD* object with the following attributes:\n",
    "- AD_Object.val: stores the value of the AD Object. \n",
    "- AD_Object.der: stores the derivative of the AD Object.\n",
    "\n",
    "The *AD_Object* class implements all relevant dunder methods(\\__repr\\__, \\__add\\__, \\__mul\\__, etc.) as well as the elementary functions (exp, sin, cos, tan, pow, ln).\n",
    "\n",
    "Once initialized, the *AD_Object* can be used as a standard variable in python, and with each elementary function applied to it, we return a new *AD_Object* with the updated value and derivative.  \n",
    "\n",
    "### 6.1.2. AD class\n",
    "The *AD* class uses the *AD_Object* to initialize AutoDiff objects, but it takes the following three inputs:\n",
    "1. A user supplied function in string format \n",
    "2. The variable label that the function will be derived wrt. \n",
    "3. The initial variable value, used for function and derivative evaluation\n",
    "\n",
    "The main AD class attributes are:\n",
    "- AD.init_value: initial variable value for evaluation of the function and gradient.\n",
    "- AD.var_label: variable label to be used (x, y, z, ...etc), for which the derivative of the function is to be returned \n",
    "- AD.func_label: the supplied function in string format.\n",
    "- AD.val: value of evaluated function @ AD.init_value.   \n",
    "- AD.der: derivative value of supplied function @ AD.init_value.\n",
    "\n",
    "Finally, the package is designed to have minimal dependencies, with all the elementary functions to be supplied as part of the package; we envision only the *numpy* package will be needed.\n",
    "## 6.2. To be implemented\n",
    "At this point, our AutoDiff package accepts only a scaler function of a single input. We are going to implement two additional features for the package:\n",
    "### 6.2.1. Vectorized input\n",
    "The package will be extended to accept a vectorized input for points to evaluate the derivative at. Thus for the same function we can return the a vectorized AD.val and AD.der; The accepted vectorized input and saved AD.val and AD.der will be saved in numpy arrays. The code will be extended wth the following pseudo-code: \n",
    "```\n",
    "if isinstance(init_value, np.ndarray):\n",
    "    AD.val = np.array([AD_object(init).val for init in init_value])\n",
    "    AD.der = np.array([AD_object(init).der for init in init_value])    \n",
    "```\n",
    "### 6.2.2. Multiple input variables\n",
    "We will extend our package to handle functions of multiple independant variabel e.g f(x,y) and be able te return partial derivatives of such function wrt to each variable. This is still in the preleminary phase, but we will probably modifiy the AD.der attribute to accept the label as an input, so we can call it f.der('x') would give us the derivative wrt 'x', ..etc. The derivative could be installed in a dictionary, where it would be asy to extract the value marked by variable keys. \n",
    "\n",
    "### 6.2.3. Elementary functions\n",
    "As stated earlier, we are still in the process of adding more elementary functions and we envision, by the end of the project, to have added most functions implemented in the math module of the standard python library.  This includes sqrt, tanh, sinh, cosh, ..etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Future Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1. Root Finding Algorithm: \n",
    "As a showcase application of the AutoDiff package, we will implement a root finding algorithm that can be simply accessed as **AD.root('x')**. As a starter, we will implement the newton-raphson algorithm and supply it with derivateves calulated by our packges; we can later expand and add more methods. Our goal is to have the AutoDiff package for a supplied function, give the user the function value, derivative and roots.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
