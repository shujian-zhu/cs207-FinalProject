{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 2 Documentation\n",
    "\n",
    "Group 24: Jessica A Wijaya, Shujian Zhu, Malik Wagih, William Palmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "Evaluation of derivatives is integral to many machine learning methods. For this purpose, two main methods could be used: symbolical and numerical differentiation. Symbolical differentiation, though straightforward, its implementation requires complex expression manipulation in computer algebra systems, making it very costly to evaluate; the method is also limited to closed-form input expressions. On the other hand, numerical differentiation computes the function derivative by approximating it using small values of step size h; though numerical simpler and faster than symbolic diffferentiation, it suffers from stability issues and round-off or truncation errors.\n",
    "\n",
    "To address the weaknesses of both these methods, automatic differentiation (AD) was introduced. Since, it has been applied in different areas, such as  engineering design optimization, structural mechanics, and atmospheric sciences; its application to machine learning methods popularised AD. Therefore, due to the important role AD plays in many scientific fields, we introduce a python package that provides user-friendly methods for performing forward-mode AD. Our package supports the evaluation of first derivatives of functions defined by user at given input value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Background\n",
    "All numerical computation can be seen as a combination of elementary operations for which the derivatives are known. The derivatives of the overall composition can be found by combining the derivatives of elementary operations through the chain rule. Such elementary functions include arithmetic operations (addition, subtraction, multiplication, division), sign switch, and functions such as exponential, logarithm, and the trigonometric (e.g. sin(x), cos(x)). Traces of these elementary operations can be represented by a trace table or a computational graph. Trace table are originally used to trace the value of variables as each line of code is executed. As an example of this flow, Table 1 shows the evaluation trace of elementary operations of the computation f(x<sub>1</sub>) = ln(x<sub>1</sub>) + 3 * x<sub>1</sub> and Figure 1 gives an example of a graphic representation of function f(x<sub>1</sub>) by its elementary operations. \n",
    "\n",
    "<p style=\"text-align: center;\"> Table 1: Evaluation Trace for a sample function</p>\n",
    "\n",
    "\n",
    "| Trace       | Elementary function | Current Function Value | Function Derivative |\n",
    "| ------------- |:-------------:|:-------------:|:-------------:|\n",
    "| X1      | X1            |  c             | 1  |\n",
    "| X2      | ln(X1)            | ln(c)      | 1/c |\n",
    "| X3      | 3 * X1            |  3c        | 3   |\n",
    "| X4      | X2 + X3             | ln(c) + 3c | 1/c + 3 |\n",
    "\n",
    "![alt text](sample_trace_graph.png \"Figure 1\")\n",
    "<p style=\"text-align: center;\"> Figure 1: Graphic representation of elementary functions sequence of a function </p>\n",
    "\n",
    "## 2.1. Forward Mode\n",
    "\n",
    "The forward mode of AD starts from the input value and compute the derivative of intermediate variables with respect to the input value. Applying the chain rule to each elementary operation in the forward primal trace, we generate the corresponding derivative trace, which gives us the derivative in the final variable. Forward mode AD can also be viewed as evaluating a function using dual numbers, which are defined as $a+b\\epsilon$, where $a, b \\in \\mathbf{R}$ and $\\epsilon$ is a nilpotent number such that $\\epsilon^2 = 0$ and $\\epsilon \\neq 0$. It can be shown that the coefficient of $\\epsilon$ after evaluating a function is exactly the derivative of that function, which also works for chain rule.\n",
    "\n",
    "The Forward mode can be considered as the computation of the Jacobian-Vector product. Given $F : R^n → R^m$ and the Jacobian $J = DF(x) ∈ R^{m×n}$.\n",
    "\n",
    "![Figure 2](JacobianMatrix.png)\n",
    "\n",
    "One sweep of the <span style=\"color:blue\"> forward mode </span> can calculate one column vector of the Jacobian, <span style=\"color:blue\"> $J \\hat{x} $ </span>, where $ \\hat{x} $ is a column vector of seeds. In comparison, one sweep of the <span style=\"color:red\"> reverse mode </span> can calculate one row vector of the Jacobian, <span style=\"color:red\"> $\\overline{y} J $ </span>, where $ \\overline{y} $ is a row vector of seeds. This is why the <span style=\"color:blue\"> forward mode </span> is very efficient to compute $F : R → R^m$, while the <span style=\"color:red\"> reverse mode </span> best suited to compute $G : R^m → R$. While the efficiency of each sweep for the forward and reverse mode is the same, the reverse mode requires access to intermediate variables thereby needing more memory, while forward mode does not come with this baggage. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Installation\n",
    "Th package uses a setup.py file for a simple installation, you need to to run the following command in the package main folder:\n",
    "\n",
    "```\n",
    "pip install .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. How to Use AutoDiff \n",
    "In this section, we demo the the AutoDiff package and its capabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import AutoDiff as ad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AutoDiff package can be used in two different ways:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Pass the function to AD()\n",
    "```\n",
    "AD(function: str, variable_label:str, variable_value: float): -> (value, derivative)\n",
    "```\n",
    "The user can initialize an *AD* class, which takes the following three inputs:\n",
    "- **function of interest (*string)** : The function has to be passed in as a string in the form that python would be able to run (e.g. using '\\*' for multiplication, using '\\*\\*' for power, etc.) For example, if the function is 3x<sup>2</sup>, the user has to pass it as '3\\*x\\*\\*2'\n",
    "- **the variable (*string)**: this is the variable for which the function derivative will be calculated in respect to, passed in as a string, e.g. 'x' or 'y'. Normally, this variable is expected to be contained in the function passed in earlier (otherwise, the function is then just a constant, with a derivative of 0)\n",
    "- **value to be evaluated at (*float)**: the package will then compute the function value and the derivative when the variable is equal to this value\n",
    "\n",
    "For example, if we are trying to evaluate the value and derivative of $f(x) = -x^3 + 2x^2 + 3x + 5$ at **x = 2**, then the inputs are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val:  11 \n",
      "der:  -1\n",
      "AD Object: Value = 11.000, Derivative =-1.000\n"
     ]
    }
   ],
   "source": [
    "## y = f(x)\n",
    "y = ad.AD('-x**3 + 2*x**2 + 3*x + 5', 'x', 2)\n",
    "print(\"val: \", y.val,\"\\nder: \", y.der)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two additional examples are provided below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val:  11 \n",
      "der:  -1\n",
      "AD Object: Value = 11.000, Derivative =-1.000\n"
     ]
    }
   ],
   "source": [
    "## z = f(w)\n",
    "z = ad.AD('-w**3 + 2*w**2 + 3*w + 5', 'w', 2)\n",
    "print(\"val: \", z.val,\"\\nder: \", z.der)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AD Object: Value = 1.000, Derivative =-0.000\n"
     ]
    }
   ],
   "source": [
    "## Elementary function example  \n",
    "y = ad.AD('cos(x)', 'x', 0)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Create an AD_Object()\n",
    "Alternatively, you can delcare an AD_Object(value) and use it as a variable; the AD object will store the value and derivative with each subsequent elementary operation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val:  11 \n",
      "der:  -1\n",
      "AD Object: Value = 11.000, Derivative =-1.000\n"
     ]
    }
   ],
   "source": [
    "x = ad.AD_Object(2)\n",
    "y = -x**3 + 2*x**2 + 3*x + 5\n",
    "print(\"val: \", y.val,\"\\nder: \", y.der)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val:  11 \n",
      "der:  -1\n",
      "AD Object: Value = 11.000, Derivative =-1.000\n"
     ]
    }
   ],
   "source": [
    "w = ad.AD_Object(2)\n",
    "z = -w**3 + 2*w**2 + 3*w + 5\n",
    "print(\"val: \", z.val,\"\\nder: \", z.der)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AD Object: Value = 1.000, Derivative =-0.000\n"
     ]
    }
   ],
   "source": [
    "## Elementary function example  \n",
    "x = ad.AD_Object(0)\n",
    "y = ad.cos(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Supported Elementary Functions\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following elementary functions are currently supported by the AutoDiff package:\n",
    "- addition(+), subtraction(-), multiplication(*) and division(/)\n",
    "- power (can be called by pow() or using **)\n",
    "- sine (sin), cosine (cos), tangent (tan)\n",
    "- natural log (ln), exponential (**note: exp should be called by e() and not exp()**)\n",
    "\n",
    "We will add more functions as the project proceeds; we envision having most functions implemented in the math module of the standard python library.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Software Organization\n",
    "\n",
    "The package has the following directory structure:\n",
    "- README.md\n",
    "- LICENSE.md\n",
    "- setup.py\n",
    "- requirements.txt\n",
    "- test/ (using CodeCov and TravisCI)\n",
    "    - \\__init\\__.py\n",
    "    - test_AutoDiff.py\n",
    "- docs\n",
    "    - milestone1.md\n",
    "    - milestone2.ipynb\n",
    "    - sample_trace_graph.png\n",
    "- AutoDiff/\n",
    "    - \\__init\\__.py\n",
    "    - AutoDiff_Class.py\n",
    "- extension/ (to be decided and implemented later)\n",
    "    - optimization.py\n",
    "    - rootfinder.py\n",
    "    - visualization/\n",
    "        - index.html\n",
    "        - js/ main.js\n",
    "        - css/ style.css\n",
    "        - ...\n",
    "\n",
    "The core auto-differentiation class will be in the autodiff.py file, which contain the class AD and class AD_Object (see below section for details on how these classes are implemented and used). \n",
    "\n",
    "The folder tests/ will contain all the files needed for testing purposes, including the file test.py that contain the codes used to test the core auto-differentiation class. We are using TravisCI and CodeCov for integeration and coverage testing; their status badges will be included in the README.md file to display the coverages. \n",
    "\n",
    "The package uses standard .py standalone files and setup.py for package installation; it will be also distributed through\n",
    "PyPi in the future. \n",
    "\n",
    "Finally, in the next iteration, we will also implement an additional functional feature, which will be contained in the folder extension. This extended module can be an additional class developed to be used by the user for finding the roots of a given function, or an animated illustration of the forward mode auto-differentiation displayed in a webpage, etc. At this point, we have not decided what this additional feature is, but the structure of the module will follow the current software organization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Implementation\n",
    "\n",
    "## 6.1. Current implementation\n",
    "\n",
    "The core class will be the class 'AD_Object', with the main object attributes being the (1) function value and (2) derivative value, both in numeric data tyoe (integer/float). These attributes' value(s) can be obtained by calling .val and .der respectively. \n",
    "\n",
    "We created the additional class 'AD' as the helper class that will be initialized by the user: when the user creates an instance of the AD class and passes the inputs (function string, variable(s), and value(s) to be evaluated at), this AD object initialization will (1) evaluate the string that the user passed in (using eval function), and (2) create instance(s) of AD_Object. \n",
    "\n",
    "Every variable that is passed by the user and is contained in the function given by the user will be initialized as an AD_Object object, and every operation done on this object will create another AD_Object object with an updated function value and derivative. This is all done by overiding the dunder functions, including the basic operations for addition, multiplication, subtraction, and division  (e.g. \\__add__, \\__radd__, \\__mul__, \\__rmul__, etc.). Our implementation on these functions will be able to handle two types of input: AD_Object object(s) and scalar. We also override the dunder methods for power and negation. For the elementary functions, we implemented the relevent functions (such as $exp(x)$, $sin(x)$, $cos(x)$, etc.) and evaluated the value by using the math module of the Python Standard Library. \n",
    "\n",
    "As a consequence of the way we developed the elementary functions (such as $exp(x)$, $sin(x)$, $cos(x)$, etc.) as AD_Object class methods, they need to be called as 'x.sin()' instead of 'sin(x)'. To help solve this issue, we then created helper functions that will be called when the user-defined-functions contains $sin(x)$ or $cos(x)$ or $exp(x)$, etc.; these functions will then call the AD_Object class methods where all the main computation take place. In addition, we also created additional functions to handle power rule, product rule, and quotientrule, as helper functions to solve more complicated operations. \n",
    "\n",
    "At this point, there is no dependencies yet. We just need to import the math module, which comes with python standard library. The package is designed to have minimal dependencies, with all the elementary functions to be supplied as part of the package; we envision only the *numpy* package will be needed in the future.\n",
    "\n",
    "**The following is a summary of the the inputs and main attributes for the each of the two classes:**\n",
    "\n",
    "### 6.1.1. AD_Object class\n",
    "The *AD_Object* class takes a numeric value as an input and intitializes an *AD* object with the following attributes\n",
    "\n",
    "- AD_Object.val: stores the value of the AD_Object\n",
    "- AD_Object.der: stores the derivative of the AD_Object.\n",
    "\n",
    "### 6.1.2. AD class\n",
    "The *AD* class uses the *AD_Object* to initialize AutoDiff objects, but it takes the following three inputs:\n",
    "1. A user supplied function in string format.\n",
    "2. The variable label that the function will be derived wrt.\n",
    "3. The initial variable value, used for function and derivative evaluation.\n",
    "\n",
    "The main AD class attributes are:\n",
    "- AD.init_value: initial variable value for evaluation of the function and gradient.\n",
    "- AD.var_label: variable label to be used (x, y, z, ...etc), for which the derivative of the function is to be returned.\n",
    "- AD.func_label: the supplied function in string format.\n",
    "- AD.val: value of evaluated function @ AD.init_value. \n",
    "- AD.der: derivative of function @ AD.init_value.\n",
    "\n",
    "\n",
    "## 6.2. To be implemented\n",
    "At this point, our AutoDiff package accepts only a scaler function of a single input. We are going to implement two additional features for the package:\n",
    "\n",
    "### 6.2.1. Vectorized input\n",
    "The package will be extended to accept a vectorized input for points to evaluate the derivative at. Thus for the same function we can return the a vectorized AD.val and AD.der; The accepted vectorized input and saved AD.val and AD.der will be saved in numpy arrays. The code will be extended wth the following pseudo-code: \n",
    "```\n",
    "if isinstance(init_value, np.ndarray):\n",
    "    AD.val = np.array([AD_object(init).val for init in init_value])\n",
    "    AD.der = np.array([AD_object(init).der for init in init_value])    \n",
    "```\n",
    "### 6.2.2. Multiple input variables\n",
    "We will extend our package to handle functions of multiple independant variabel e.g f(x,y) and be able te return partial derivatives of such function wrt to each variable. This is still in the preleminary phase, but we will probably modifiy the AD.der attribute to accept the label as an input, so we can call it f.der('x') would give us the derivative wrt 'x', ..etc. The derivative could be installed in a dictionary, where it would be asy to extract the value marked by variable keys. \n",
    "\n",
    "### 6.2.3. Elementary functions\n",
    "As stated earlier, we are still in the process of adding more elementary functions and we envision, by the end of the project, to have added most functions implemented in the math module of the standard python library.  This includes sqrt, tanh, sinh, cosh, ..etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Future Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1. Root Finding Algorithm: \n",
    "As a showcase application of the AutoDiff package, we will implement a root finding algorithm that can be simply accessed as **AD.root('x')**. As a starter, we will implement the newton-raphson algorithm and supply it with derivateves calulated by our packges; we can later expand and add more methods. Our goal is to have the AutoDiff package for a supplied function, give the user the function value, derivative and roots.     "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
